---
layout: paper
title: References
---

<div class="references">
  <h2>Bibliography</h2>
  <div class="reference-list">
    <div class="reference-item">
      <div class="reference-text">
        Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903.
      </div>
      <div class="reference-links">
        <a href="https://arxiv.org/abs/2201.11903" target="_blank">arXiv</a>
        <a href="#" target="_blank">Google Scholar</a>
      </div>
    </div>
    
    <div class="reference-item">
      <div class="reference-text">
        Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4 technical report, 2024. URL https://arxiv.org/abs/2412.08905.
      </div>
      <div class="reference-links">
        <a href="https://arxiv.org/abs/2412.08905" target="_blank">arXiv</a>
        <a href="#" target="_blank">Google Scholar</a>
      </div>
    </div>
    
    <div class="reference-item">
      <div class="reference-text">
        Zicheng Lin, Tian Liang, Jiahao Xu, Qiuzhi Lin, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, and Zhaopeng Tu. Critical tokens matter: Token-level contrastive estimation enhances llm's reasoning capability, 2025. URL https://arxiv.org/abs/2411.19943.
      </div>
      <div class="reference-links">
        <a href="https://arxiv.org/abs/2411.19943" target="_blank">arXiv</a>
        <a href="#" target="_blank">Google Scholar</a>
      </div>
    </div>
    
    <div class="reference-item">
      <div class="reference-text">
        Asankhaya Sharma. Qwen3-0.6b-pts, 2025a. URL https://huggingface.co/datasets/codelion/Qwen3-0.6B-pts.
      </div>
      <div class="reference-links">
        <a href="https://huggingface.co/datasets/codelion/Qwen3-0.6B-pts" target="_blank">Hugging Face</a>
        <a href="#" target="_blank">Google Scholar</a>
      </div>
    </div>
    
    <div class="reference-item">
      <div class="reference-text">
        Ruixuan Huang. Steering llms' behavior with concept activation vectors, September 2024. Draft manuscript. Available on LessWrong forum.
      </div>
      <div class="reference-links">
        <a href="#" target="_blank">LessWrong</a>
        <a href="#" target="_blank">Google Scholar</a>
      </div>
    </div>
    
    <div class="reference-item">
      <div class="reference-text">
        Nishant Subramani, Nivedita Suresh, and Matthew E. Peters. Extracting latent steering vectors from pretrained language models, 2022. URL https://arxiv.org/abs/2205.05124.
      </div>
      <div class="reference-links">
        <a href="https://arxiv.org/abs/2205.05124" target="_blank">arXiv</a>
        <a href="#" target="_blank">Google Scholar</a>
      </div>
    </div>
    
    <div class="reference-item">
      <div class="reference-text">
        Beier Zhu, Jiequan Cui, Hanwang Zhang, and Chi Zhang. Project-probe-aggregate: Efficient fine-tuning for group robustness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. URL https://arxiv.org/abs/2503.09487.
      </div>
      <div class="reference-links">
        <a href="https://arxiv.org/abs/2503.09487" target="_blank">arXiv</a>
        <a href="#" target="_blank">Google Scholar</a>
      </div>
    </div>
    
    <div class="reference-item">
      <div class="reference-text">
        Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà, and Mikhail Belkin. Toward universal steering and monitoring of ai models, 2025. URL https://arxiv.org/abs/2502.03708v1.
      </div>
      <div class="reference-links">
        <a href="https://arxiv.org/abs/2502.03708v1" target="_blank">arXiv</a>
        <a href="#" target="_blank">Google Scholar</a>
      </div>
    </div>
    
    <div class="reference-item">
      <div class="reference-text">
        Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes, 2018. URL https://arxiv.org/abs/1610.01644.
      </div>
      <div class="reference-links">
        <a href="https://arxiv.org/abs/1610.01644" target="_blank">arXiv</a>
        <a href="#" target="_blank">Google Scholar</a>
      </div>
    </div>
    
    <div class="reference-item">
      <div class="reference-text">
        Trenton Bricken and Others. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits blog, 2023. URL https://transformer-circuits.pub/2023/monosemantic-features/index.html.
      </div>
      <div class="reference-links">
        <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html" target="_blank">Transformer Circuits</a>
        <a href="#" target="_blank">Google Scholar</a>
      </div>
    </div>
    
    <div class="reference-item">
      <div class="reference-text">
        Asankhaya Sharma. Pts: Pivotal token search, 2025b. URL https://github.com/codelion/pts.
      </div>
      <div class="reference-links">
        <a href="https://github.com/codelion/pts" target="_blank">GitHub</a>
        <a href="#" target="_blank">Google Scholar</a>
      </div>
    </div>
    
    <div class="reference-item">
      <div class="reference-text">
        Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superposition, September 2022. URL https://arxiv.org/abs/2209.10652. Submitted on 21 Sep 2022.
      </div>
      <div class="reference-links">
        <a href="https://arxiv.org/abs/2209.10652" target="_blank">arXiv</a>
        <a href="#" target="_blank">Google Scholar</a>
      </div>
    </div>
  </div>
</div>

<h2>Additional Resources</h2>
<p>Links to additional resources related to your research:</p>
<ul>
  <li><a href="https://huggingface.co/datasets/codelion/Qwen3-0.6B-pts" target="_blank">PTS Dataset</a></li>
  <li><a href="https://github.com/codelion/pts" target="_blank">PTS Code Repository</a></li>
  <li><a href="https://transformer-circuits.pub/" target="_blank">Transformer Circuits Blog</a></li>
  <li><a href="https://lesswrong.com/" target="_blank">LessWrong Forum</a></li>
</ul>
